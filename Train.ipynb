{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please use python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 11.9 Âµs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from model import UniSkip\n",
    "from config import *\n",
    "from datetime import datetime, timedelta\n",
    "%time\n",
    "%load_ext autotime\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text file at ../dir_HugeFiles/instructions/skip_instruction.csv\n",
      "Making dictionary for these words\n",
      "Using cached dictionary at ../dir_HugeFiles/instructions/skip_instruction.csv.pkl\n",
      "Making reverse dictionary\n",
      "time: 330 ms\n"
     ]
    }
   ],
   "source": [
    "d = DataLoader('../dir_HugeFiles/instructions/skip_instruction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.8 s\n"
     ]
    }
   ],
   "source": [
    "mod = UniSkip()\n",
    "if USE_CUDA:\n",
    "    mod.cuda(CUDA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.2 ms\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-4\n",
    "optimizer = torch.optim.Adam(params=mod.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 30.9 ms\n"
     ]
    }
   ],
   "source": [
    "loss_trail = []\n",
    "last_best_loss = None\n",
    "current_time = datetime.utcnow()\n",
    "\n",
    "def debug(i, loss, prev, nex, prev_pred, next_pred):\n",
    "    global loss_trail\n",
    "    global last_best_loss\n",
    "    global current_time\n",
    "\n",
    "    this_loss = loss.item()\n",
    "    loss_trail.append(this_loss)\n",
    "    loss_trail = loss_trail[-20:]\n",
    "    new_current_time = datetime.utcnow()\n",
    "    time_elapsed = str(new_current_time - current_time)\n",
    "    current_time = new_current_time\n",
    "    print(\"Iteration {}: time = {} last_best_loss = {}, this_loss = {}\".format(\n",
    "              i, time_elapsed, last_best_loss, this_loss))\n",
    "    \n",
    "    '''\n",
    "    print(\"prev = {}\\nnext = {}\\npred_prev = {}\\npred_next = {}\".format(\n",
    "        d.convert_indices_to_sentences(prev),\n",
    "        d.convert_indices_to_sentences(nex),\n",
    "        d.convert_indices_to_sentences(prev_pred),\n",
    "        d.convert_indices_to_sentences(next_pred),\n",
    "    ))\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        trail_loss = sum(loss_trail)/len(loss_trail)\n",
    "        if last_best_loss is None or last_best_loss > trail_loss:\n",
    "            print(\"Loss improved from {} to {}\".format(last_best_loss, trail_loss))\n",
    "            \n",
    "            save_loc = \"./saved_models/skip-best\".format(lr, VOCAB_SIZE)\n",
    "            print(\"saving model at {}\".format(save_loc))\n",
    "            torch.save(mod.state_dict(), save_loc)\n",
    "            \n",
    "            last_best_loss = trail_loss\n",
    "    except Exception as e:\n",
    "        print(\"Couldn't save model because {}\".format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "current GPU is on 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: time = 0:00:00.456559 last_best_loss = None, this_loss = 19.825910568237305\n",
      "Loss improved from None to 19.825910568237305\n",
      "saving model at ./saved_models/skip-best\n",
      "Iteration 100: time = 0:01:17.574393 last_best_loss = 19.825910568237305, this_loss = 14.4572172164917\n",
      "Loss improved from 19.825910568237305 to 17.141563892364502\n",
      "saving model at ./saved_models/skip-best\n",
      "Iteration 200: time = 0:01:16.605824 last_best_loss = 17.141563892364502, this_loss = 13.789891242980957\n",
      "Loss improved from 17.141563892364502 to 16.02433967590332\n",
      "saving model at ./saved_models/skip-best\n",
      "Iteration 300: time = 0:01:16.859282 last_best_loss = 16.02433967590332, this_loss = 13.485515594482422\n",
      "Loss improved from 16.02433967590332 to 15.389633655548096\n",
      "saving model at ./saved_models/skip-best\n",
      "Iteration 400: time = 0:01:16.547689 last_best_loss = 15.389633655548096, this_loss = 12.589254379272461\n",
      "Loss improved from 15.389633655548096 to 14.829557800292969\n",
      "saving model at ./saved_models/skip-best\n",
      "Iteration 500: time = 0:01:13.882864 last_best_loss = 14.829557800292969, this_loss = 12.193212509155273\n",
      "Loss improved from 14.829557800292969 to 14.390166918436686\n",
      "saving model at ./saved_models/skip-best\n",
      "Iteration 600: time = 0:01:15.396194 last_best_loss = 14.390166918436686, this_loss = 12.251800537109375\n",
      "Loss improved from 14.390166918436686 to 14.0846860068185\n",
      "saving model at ./saved_models/skip-best\n",
      "Iteration 700: time = 0:01:16.707801 last_best_loss = 14.0846860068185, this_loss = 12.88994312286377\n",
      "Loss improved from 14.0846860068185 to 13.935343146324158\n",
      "saving model at ./saved_models/skip-best\n",
      "Iteration 800: time = 0:01:15.416624 last_best_loss = 13.935343146324158, this_loss = 12.184877395629883\n",
      "Loss improved from 13.935343146324158 to 13.740846951802572\n",
      "saving model at ./saved_models/skip-best\n",
      "Iteration 900: time = 0:01:14.804112 last_best_loss = 13.740846951802572, this_loss = 12.754266738891602\n",
      "Loss improved from 13.740846951802572 to 13.642188930511475\n",
      "saving model at ./saved_models/skip-best\n",
      "Iteration 1000: time = 0:01:16.786304 last_best_loss = 13.642188930511475, this_loss = 12.139989852905273\n",
      "Loss improved from 13.642188930511475 to 13.50562537800182\n",
      "saving model at ./saved_models/skip-best\n",
      "Iteration 1100: time = 0:01:16.145769 last_best_loss = 13.50562537800182, this_loss = 12.285396575927734\n",
      "Loss improved from 13.50562537800182 to 13.403939644495646\n",
      "saving model at ./saved_models/skip-best\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-68185eddab78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14min 9s\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "print('current GPU is on %d ' %(CUDA_DEVICE)) # should change at the config.py\n",
    "# a million iterations\n",
    "for i in range(0, 1000000):\n",
    "    sentences, lengths = d.fetch_batch(32 * 8) # remember to change cuda device\n",
    "\n",
    "    loss, prev, nex, prev_pred, next_pred  = mod(sentences, lengths)\n",
    "    if i % 100 == 0:\n",
    "        debug(i, loss, prev, nex, prev_pred, next_pred)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
